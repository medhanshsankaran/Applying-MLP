{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad79351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f527b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data loading\n",
    "# -----------------------------\n",
    "df = pd.read_csv('Data/project_adult.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78921b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process and standardize data\n",
    "# This is not an all inclusive function....you should improve!\n",
    "def preprocess_data(df):\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Drop unnamed column if it exists\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "    # split off target early (if present)\n",
    "    y = None\n",
    "    if 'income' in df.columns:\n",
    "        y = df['income']\n",
    "        df = df.drop(columns=['income'])\n",
    "    \n",
    "    if y is not None:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y).astype(int)  # encode target as int\n",
    "\n",
    "    # detect numeric vs categorical by attempted coercion\n",
    "    numeric_cols, categorical_cols = [], []\n",
    "    for col in df.columns:\n",
    "        coerced = pd.to_numeric(df[col], errors='coerce')\n",
    "        if coerced.notna().all():\n",
    "            df[col] = coerced.astype(float)   # numeric as float\n",
    "            numeric_cols.append(col)\n",
    "        else:\n",
    "            df[col] = df[col].astype(str)     # categorical as string\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    # label-encode categoricals only\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col]).astype(int)  # encoded as int\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # standardize numeric columns only\n",
    "    scaler = StandardScaler()\n",
    "    if numeric_cols:\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols].astype(float)).astype(float)\n",
    "\n",
    "    # Encode categorical features\n",
    "    label_encoders = {}\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "\n",
    "    # Make X and y arrays\n",
    "    X = df.values.astype(float)\n",
    "    if y is not None:\n",
    "        y = y.astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e323a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- activations and derivatives ---\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def dsigmoid(a):\n",
    "    # derivative in terms of activation output a = sigmoid(z)\n",
    "    return a * (1. - a)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def dtanh(a):\n",
    "    # derivative in terms of activation output a = tanh(z)\n",
    "    return 1. - a**2\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0.0, z)\n",
    "\n",
    "def drelu(z):\n",
    "    # derivative needs pre-activation z for ReLU\n",
    "    out = np.zeros_like(z)\n",
    "    out[z > 0] = 1.0\n",
    "    return out\n",
    "\n",
    "def softmax(z):\n",
    "    # numerically stable softmax along last axis\n",
    "    z_shift = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_shift)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# --- utils ---\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "    return ary\n",
    "\n",
    "# --- model ---\n",
    "\n",
    "class NeuralNetMLP:\n",
    "    \"\"\"\n",
    "    Two-layer MLP with pluggable hidden/output activations.\n",
    "    Hidden activations supported: 'sigmoid', 'tanh', 'relu'\n",
    "    Output activations supported: 'sigmoid', 'tanh', 'relu', 'softmax'\n",
    "    Loss:\n",
    "      - For output='softmax' uses multi-class cross-entropy by default\n",
    "      - Otherwise uses mean squared error by default\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes,\n",
    "                 hidden_activation='sigmoid',\n",
    "                 output_activation='sigmoid',\n",
    "                 loss=None,\n",
    "                 random_seed=123):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_activation_name = hidden_activation.lower()\n",
    "        self.output_activation_name = output_activation.lower()\n",
    "\n",
    "        # choose loss if not provided\n",
    "        if loss is None:\n",
    "            self.loss_name = 'cross_entropy' if self.output_activation_name == 'softmax' else 'mse'\n",
    "        else:\n",
    "            self.loss_name = loss.lower()\n",
    "\n",
    "        # init params\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        self.weight_h = rng.normal(loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "\n",
    "        self.weight_out = rng.normal(loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "\n",
    "    # --- activation dispatch ---\n",
    "\n",
    "    def _act_hidden(self, z_h):\n",
    "        if self.hidden_activation_name == 'sigmoid':\n",
    "            a = sigmoid(z_h)\n",
    "            cache = a  # derivative uses activation\n",
    "            return a, cache\n",
    "        elif self.hidden_activation_name == 'tanh':\n",
    "            a = tanh(z_h)\n",
    "            cache = a\n",
    "            return a, cache\n",
    "        elif self.hidden_activation_name == 'relu':\n",
    "            a = relu(z_h)\n",
    "            cache = z_h  # derivative uses pre-activation for ReLU\n",
    "            return a, cache\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported hidden activation: {self.hidden_activation_name}\")\n",
    "\n",
    "    def _dact_hidden(self, cache):\n",
    "        if self.hidden_activation_name == 'sigmoid':\n",
    "            return dsigmoid(cache)\n",
    "        elif self.hidden_activation_name == 'tanh':\n",
    "            return dtanh(cache)\n",
    "        elif self.hidden_activation_name == 'relu':\n",
    "            return drelu(cache)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported hidden activation: {self.hidden_activation_name}\")\n",
    "\n",
    "    def _act_out(self, z_out):\n",
    "        if self.output_activation_name == 'sigmoid':\n",
    "            a = sigmoid(z_out)\n",
    "            cache = a\n",
    "            return a, cache\n",
    "        elif self.output_activation_name == 'tanh':\n",
    "            a = tanh(z_out)\n",
    "            cache = a\n",
    "            return a, cache\n",
    "        elif self.output_activation_name == 'relu':\n",
    "            a = relu(z_out)\n",
    "            cache = z_out\n",
    "            return a, cache\n",
    "        elif self.output_activation_name == 'softmax':\n",
    "            a = softmax(z_out)\n",
    "            cache = a  # for softmax+CE, derivative uses a directly\n",
    "            return a, cache\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output activation: {self.output_activation_name}\")\n",
    "\n",
    "    def _dact_out(self, cache):\n",
    "        if self.output_activation_name == 'sigmoid':\n",
    "            return dsigmoid(cache)\n",
    "        elif self.output_activation_name == 'tanh':\n",
    "            return dtanh(cache)\n",
    "        elif self.output_activation_name == 'relu':\n",
    "            return drelu(cache)\n",
    "        elif self.output_activation_name == 'softmax':\n",
    "            # with cross-entropy, softmax derivative handled directly in backward\n",
    "            # this function won't be used in that case\n",
    "            raise RuntimeError(\"Do not call _dact_out for softmax; handled directly with cross-entropy.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output activation: {self.output_activation_name}\")\n",
    "\n",
    "    # --- forward/backward ---\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden layer: z_h -> a_h\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h, hidden_cache = self._act_hidden(z_h)\n",
    "\n",
    "        # output layer: z_out -> a_out\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        a_out, out_cache = self._act_out(z_out)\n",
    "\n",
    "        cache = {\n",
    "            'x': x,\n",
    "            'z_h': z_h,\n",
    "            'a_h': a_h,\n",
    "            'hidden_cache': hidden_cache,\n",
    "            'z_out': z_out,\n",
    "            'a_out': a_out,\n",
    "            'out_cache': out_cache\n",
    "        }\n",
    "        return a_h, a_out, cache\n",
    "\n",
    "    def backward(self, y, cache):\n",
    "        \"\"\"\n",
    "        Computes gradients for weights and biases using the chosen activations and loss.\n",
    "        y is class indices of shape [n_examples].\n",
    "        \"\"\"\n",
    "        x = cache['x']\n",
    "        a_h = cache['a_h']\n",
    "        a_out = cache['a_out']\n",
    "\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "        N = y.shape[0]\n",
    "\n",
    "        # delta_out based on loss and output activation\n",
    "        if self.loss_name == 'cross_entropy' and self.output_activation_name == 'softmax':\n",
    "            # gradient wrt logits for softmax+CE\n",
    "            delta_out = (a_out - y_onehot) / N\n",
    "        elif self.loss_name == 'mse':\n",
    "            # MSE: dL/da = 2*(a - y)/N; then multiply by activation derivative wrt logits\n",
    "            d_loss__d_a_out = 2.0 * (a_out - y_onehot) / N\n",
    "            if self.output_activation_name == 'softmax':\n",
    "                # not typical, but provide a safe fallback via full Jacobian-vector product\n",
    "                # J_softmax = diag(a) - a a^T ; compute row-wise\n",
    "                delta_out = np.empty_like(a_out)\n",
    "                for i in range(N):\n",
    "                    a = a_out[i:i+1]          # shape (1, C)\n",
    "                    v = d_loss__d_a_out[i:i+1]  # shape (1, C)\n",
    "                    jac = np.diagflat(a) - a.T @ a  # (C, C)\n",
    "                    delta_out[i] = (v @ jac).ravel()\n",
    "            else:\n",
    "                d_a_out__d_z_out = self._dact_out(cache['out_cache'])\n",
    "                delta_out = d_loss__d_a_out * d_a_out__d_z_out\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss/activation combo: loss={self.loss_name}, out_act={self.output_activation_name}\")\n",
    "\n",
    "        # gradients for output layer\n",
    "        d_loss__dw_out = np.dot(delta_out.T, a_h)           # (num_classes, num_hidden)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)          # (num_classes,)\n",
    "\n",
    "        # backprop to hidden activations\n",
    "        d_loss__a_h = np.dot(delta_out, self.weight_out)    # (N, num_hidden)\n",
    "\n",
    "        # hidden activation derivative\n",
    "        d_a_h__d_z_h = self._dact_hidden(cache['hidden_cache'])\n",
    "\n",
    "        # gradients for hidden layer\n",
    "        delta_h = d_loss__a_h * d_a_h__d_z_h                 # (N, num_hidden)\n",
    "        d_loss__d_w_h = np.dot(delta_h.T, x)                 # (num_hidden, num_features)\n",
    "        d_loss__d_b_h = np.sum(delta_h, axis=0)              # (num_hidden,)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out,\n",
    "                d_loss__d_w_h, d_loss__d_b_h)\n",
    "\n",
    "    # optional helper: compute loss for monitoring\n",
    "    def loss(self, y, a_out):\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "        if self.loss_name == 'cross_entropy' and self.output_activation_name == 'softmax':\n",
    "            # small epsilon for stability\n",
    "            eps = 1e-12\n",
    "            probs = np.clip(a_out, eps, 1.0 - eps)\n",
    "            # mean negative log likelihood\n",
    "            return -np.mean(np.sum(y_onehot * np.log(probs), axis=1))\n",
    "        elif self.loss_name == 'mse':\n",
    "            return np.mean((a_out - y_onehot) ** 2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss: {self.loss_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085d63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, y_true):\n",
    "    # Heatmap with consistent label order [-1, +1]\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ticks = [-1, 1]\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=ticks, yticklabels=ticks,\n",
    "           title='Confusion Matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2. if cm.max() > 0 else 0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5946e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = preprocess_data(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d96dd93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial test cross-entropy: 1.2818\n",
      "Initial test accuracy: 75.93%\n",
      "Mean |sum(probas)-1| across rows: 3.94e-17\n"
     ]
    }
   ],
   "source": [
    "# infer dimensions from the test split\n",
    "num_features = X_test.shape[1]\n",
    "num_classes = 2 # assumes labels are 0..C-1\n",
    "\n",
    "# instantiate model with the new pluggable-activation API\n",
    "model = NeuralNetMLP(\n",
    "    num_features=num_features,\n",
    "    num_hidden=50,\n",
    "    num_classes=num_classes,\n",
    "    hidden_activation='relu',      # reasonable default for hidden\n",
    "    output_activation='softmax'    # proper multi-class output\n",
    ")\n",
    "\n",
    "# forward pass on the test set\n",
    "_, probas, _ = model.forward(X_test)\n",
    "\n",
    "# for softmax outputs, the built-in loss() uses cross-entropy\n",
    "test_loss = model.loss(y_test, probas)\n",
    "\n",
    "# basic accuracy\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "test_acc = np.mean(predicted_labels == y_test)\n",
    "\n",
    "# quick sanity check: rows should sum to ~1 with softmax\n",
    "row_sum_check = np.mean(np.abs(np.sum(probas, axis=1) - 1.0))\n",
    "\n",
    "print(f\"Initial test cross-entropy: {test_loss:.4f}\")\n",
    "print(f\"Initial test accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Mean |sum(probas)-1| across rows: {row_sum_check:.2e}\")\n",
    "\n",
    "# optional: if switching to a non-softmax output with MSE, use this helper\n",
    "def mse_loss(targets, probas, num_labels=num_classes):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77247d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# --- sklearn-compatible wrapper ---\n",
    "\n",
    "class MLPClassifierSk(BaseEstimator, ClassifierMixin):\n",
    "    # keep only tunable hyperparameters as __init__ args\n",
    "    def __init__(self,\n",
    "                 num_hidden=64,\n",
    "                 hidden_activation='relu',\n",
    "                 output_activation='softmax',\n",
    "                 epochs=20,\n",
    "                 batch_size=64,\n",
    "                 lr=0.1,\n",
    "                 random_seed=123):\n",
    "        # sklearn picks these up automatically for cloning/grid search\n",
    "        self.num_hidden = num_hidden\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # non-hyperparams initialized later\n",
    "        self.model_ = None\n",
    "        self.num_classes_ = None\n",
    "        self.num_features_ = None\n",
    "\n",
    "    def _init_model(self, X, y):\n",
    "        self.num_features_ = X.shape[1]\n",
    "        self.num_classes_ = int(np.max(y)) + 1\n",
    "        self.model_ = NeuralNetMLP(\n",
    "            num_features=self.num_features_,\n",
    "            num_hidden=self.num_hidden,\n",
    "            num_classes=self.num_classes_,\n",
    "            hidden_activation=self.hidden_activation,\n",
    "            output_activation=self.output_activation,\n",
    "            random_seed=self.random_seed\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        rng = np.random.RandomState(self.random_seed)\n",
    "        y = y.astype(int)\n",
    "        self._init_model(X, y)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            idx = rng.permutation(len(X))\n",
    "            X_shuf, y_shuf = X[idx], y[idx]\n",
    "\n",
    "            for start in range(0, len(X_shuf), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                xb = X_shuf[start:end]\n",
    "                yb = y_shuf[start:end]\n",
    "\n",
    "                _, a_out, cache = self.model_.forward(xb)\n",
    "                dW_out, db_out, dW_h, db_h = self.model_.backward(yb, cache)\n",
    "\n",
    "                # SGD step\n",
    "                self.model_.weight_out -= self.lr * dW_out\n",
    "                self.model_.bias_out   -= self.lr * db_out\n",
    "                self.model_.weight_h   -= self.lr * dW_h\n",
    "                self.model_.bias_h     -= self.lr * db_h\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        _, scores, _ = self.model_.forward(X)\n",
    "        # enforce probabilities for any output activation\n",
    "        return softmax(scores)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea58bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- grid search using only X_train/y_train with internal CV, then evaluate on X_test/y_test ---\n",
    "\n",
    "# ensure labels are ints and 0..C-1\n",
    "y_train = y_train.astype(int)\n",
    "y_test  = y_test.astype(int)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb8bff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best params: {'batch_size': 32, 'epochs': 80, 'hidden_activation': 'sigmoid', 'lr': 0.05, 'num_hidden': 128, 'output_activation': 'softmax'}\n",
      "Best CV accuracy: 0.8431\n",
      "Test accuracy: 0.8401\n",
      "Mean |sum(probas)-1| on test: 9.40e-18\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'output_activation': ['softmax'],       # keep softmax for multi-class (and binary with 2 logits)\n",
    "    'num_hidden': [32, 64, 128],\n",
    "    'epochs': [20, 40, 80],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'lr': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=MLPClassifierSk(random_seed=123),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    n_jobs=1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best CV accuracy: {:.4f}\".format(gs.best_score_))\n",
    "\n",
    "best_clf = gs.best_estimator_\n",
    "test_acc = best_clf.score(X_test, y_test)\n",
    "probas_test = best_clf.predict_proba(X_test)\n",
    "row_sum_err = np.mean(np.abs(np.sum(probas_test, axis=1) - 1.0))\n",
    "\n",
    "print(\"Test accuracy: {:.4f}\".format(test_acc))\n",
    "print(\"Mean |sum(probas)-1| on test: {:.2e}\".format(row_sum_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dcf2ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Confusion Matrix ---\n",
      "[[3686  270]\n",
      " [ 563  691]]\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  <=50K (-1)       0.87      0.93      0.90      3956\n",
      "   >50K (+1)       0.72      0.55      0.62      1254\n",
      "\n",
      "    accuracy                           0.84      5210\n",
      "   macro avg       0.79      0.74      0.76      5210\n",
      "weighted avg       0.83      0.84      0.83      5210\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict on the test set using the best estimator\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['<=50K (-1)', '>50K (+1)']))\n",
    "\n",
    "plot_confusion_matrix(cm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8431c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the validation inputs:\n",
      "[-1 -1  1 ...  1 -1 -1]\n",
      "Validation inputs and predictions saved to 'validation_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Validation set using the SAME preprocessing state\n",
    "df_valid = pd.read_csv('Data/project_validation_inputs.csv', encoding='utf-8')\n",
    "X_valid, y_valid = preprocess_data(df_valid)\n",
    "\n",
    "print(\"Predictions for the validation inputs:\")\n",
    "y_pred_valid = best_model.predict(X_valid)\n",
    "\n",
    "# Convert predictions: 0 -> -1, 1 -> 1\n",
    "y_pred_valid = np.where(y_pred_valid == 0, -1, 1)\n",
    "\n",
    "print(y_pred_valid)\n",
    "\n",
    "# save results with predictions\n",
    "validation_results_df = pd.DataFrame(X_valid)\n",
    "validation_results_df['predicted_income_pm1'] = y_pred_valid\n",
    "validation_results_df.to_csv('validation_predictions.csv', index=False)\n",
    "\n",
    "print(\"Validation inputs and predictions saved to 'validation_predictions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
